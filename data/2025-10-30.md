<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 50]
- [q-bio.QM](#q-bio.QM) [Total: 5]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo协议利用群体智能和分布式成对排序共识，在AI推理中实现卓越性能，显著优于多数投票方法，并在多个基准测试中表现出更高的准确性和对抗性鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI面临计算瓶颈和大型训练收益递减，需要一种能够在容量和能力上水平扩展的推理层。

Method: 采用群体推理方法，通过同行排名、声誉加权的异构模型共识，使用成对排序和自定义Bradley-Terry聚合模型，结合链上声誉系统和能力证明机制。

Result: 在GPQA Diamond上达到85.90%准确率，比多数投票高17.21个百分点（相对提升25.1%），在对抗性提示注入攻击下仅下降0.12%，而单模型基线下降6.20%。

Conclusion: 为去中心化AI系统奠定了基础，通过集体智能实现高质量推理的民主化访问，同时保持可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [2] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文从线性CNN（弱）到两层ReLU CNN（强）的角度，分析了弱到强泛化现象，识别了基于数据集信噪比特征的两种机制：数据稀缺和数据丰富机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究大多局限于抽象框架或线性/随机特征模型，本文旨在通过更具体的CNN架构提供弱到强泛化的正式分析。

Method: 使用结构化数据（包含不同难度的标签相关信号和标签无关噪声），分析强模型在预训练弱模型标注数据上进行梯度下降训练时的动态。

Result: 在数据稀缺机制中，泛化通过良性过拟合或有害过拟合发生，并确定了过渡边界；在数据丰富机制中，泛化通过早期标签修正出现，但过度训练会降低性能。

Conclusion: 弱到强泛化在不同数据机制下表现出不同的动态特征，为理解这一现象提供了更深入的理论洞察。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [3] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析适应性景观，计算20个生物学相关特征来表征景观地形，并应用于解释和比较数十种适应性预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型基准缺乏关于底层适应性景观的地形信息，这阻碍了模型性能的解释和比较。

Method: 开发GraphFLA框架，从DNA、RNA、蛋白质等不同模态的突变数据构建适应性景观，计算20个生物学相关特征来表征4个基本方面的景观地形。

Result: 将GraphFLA应用于来自ProteinGym、RNAGym和CIS-BP的5,300多个景观，证明其在解释和比较数十种适应性预测模型性能方面的实用性，并发布了155个组合完整的经验适应性景观，包含超过220万条序列。

Conclusion: GraphFLA提供了一个强大的框架来表征和分析适应性景观地形，有助于更好地理解和比较适应性预测模型的性能。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [4] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 本文发现OOD泛化基准中观察到的ID-OOD准确率正相关（"准确率在线"现象）可能是聚合异质OOD样本的假象。通过OODSelect方法识别出语义一致的OOD子集，在这些子集中更高的ID准确率反而预测更低的OOD准确率。


<details>
  <summary>Details</summary>
Motivation: 现有OOD泛化基准中普遍观察到的ID-OOD准确率正相关模式常被解释为虚假相关性在实践中很少见，但作者怀疑这种模式可能是聚合异质OOD样本造成的假象。

Method: 提出基于梯度的OODSelect方法，用于识别语义一致的OOD子集，在这些子集中检验ID-OOD准确率关系是否仍然成立。

Result: 在广泛使用的分布偏移基准中，OODSelect识别出的子集（有时占标准OOD集一半以上）显示出更高的ID准确率预测更低的OOD准确率，与聚合指标呈现的"准确率在线"模式相反。

Conclusion: 聚合指标可能掩盖OOD鲁棒性的重要失效模式，需要更细粒度的分析来理解模型在OOD泛化中的真实表现。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [5] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现大型语言模型生成的思维链中，许多推理步骤实际上对最终预测没有因果影响，这些装饰性思考步骤只是表面上的推理过程。


<details>
  <summary>Details</summary>
Motivation: 揭示思维链中推理步骤的真实因果影响，挑战思维链忠实反映模型内部思考过程的假设，提高LLM推理效率和可信度。

Method: 提出真实思考分数(TTS)来衡量每个推理步骤对最终预测的因果影响，并识别LLMs潜在空间中的真实思考方向。

Result: 平均只有2.3%的推理步骤具有高TTS值，LLMs经常在真实思考和装饰性思考之间交替；通过控制真实思考方向可以强制模型执行或忽略特定推理步骤。

Conclusion: LLMs经常在思维链中表达推理步骤但内部并未真正执行，这削弱了LLM推理的效率和思维链的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [6] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型存在文化敏感神经元，这些神经元对不同文化背景的输入表现出选择性敏感。通过CVQA基准测试，识别出文化选择性神经元，并证明其消融会显著影响相应文化的性能表现。提出新的基于边界的CAS选择器，优于现有方法，并发现这些神经元倾向于聚集在特定解码器层。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现优异，但在处理文化相关输入时仍存在困难。研究旨在理解模型如何处理文化基础信息，探索文化敏感神经元的存在及其重要性。

Method: 使用CVQA基准测试，识别文化选择性神经元，并进行因果测试，通过消融不同识别方法标记的神经元。在25个文化群体的三个VLMs上进行实验，提出新的基于边界的对比激活选择(CAS)方法。

Result: 实验证明存在文化敏感神经元，其消融会不成比例地损害相应文化问题的性能，而对其他文化影响最小。CAS选择器在识别文化敏感神经元方面优于现有的概率和熵基方法。层间分析显示这些神经元倾向于聚集在特定解码器层。

Conclusion: 研究揭示了多模态表征的内部组织方式，为理解视觉语言模型处理文化信息提供了新的视角。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [7] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型的低维结构，发现现代语言模型在logits矩阵上表现出低秩特性，并利用这一特性通过不相关提示的线性组合来生成目标响应。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型固有的低维结构是一个重要问题，作者希望从模型无关的角度研究语言模型作为序列概率模型的低维结构。

Method: 通过实证分析展示现代语言模型logits矩阵的低秩结构，并利用这种结构通过不相关提示的线性组合来生成响应。

Result: 实验证明多种现代语言模型都表现出低秩结构，并且可以利用这种结构有效生成目标提示的响应。

Conclusion: 语言模型的低秩结构提供了一个简单的通用抽象，其理论预测与实验结果一致，并给出了可证明的学习保证。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [8] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出了一种基于博弈论的端到端特征选择框架，通过评估特征的协同作用和边际贡献来确定特征重要性，显著降低计算成本同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的指数级增长，机器学习模型训练的计算成本不断上升，但许多特征对模型性能没有积极贡献却消耗大量计算资源。

Method: 基于合作博弈论构建特征选择框架，将特征建模为玩家，通过评估协同交互和边际贡献确定特征重要性，包含样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 实验结果表明，该方法在保持预测性能的同时实现了显著的计算减少，为解决大规模机器学习的计算挑战提供了高效解决方案。

Conclusion: 该博弈论特征选择框架为大规模机器学习提供了有效的计算优化方案，在降低计算成本的同时保持了模型性能。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [9] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将每个去噪步骤视为无条件先验和状态条件策略头之间的顺序假设检验，为扩散策略添加了统计风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中具有竞争力，但在采样时通常使用缺乏统计风险概念的启发式方法进行引导。需要一种具有用户可解释风险预算的风险感知采样规则。

Method: 引入LRT-Diffusion，积累对数似然比，并使用逻辑控制器对条件均值进行门控，其阈值τ在校准后满足用户指定的Type-I水平α。训练保持标准DDPM结构，LRT引导自然与Q梯度组合。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion在实现期望α水平的同时，相比强Q引导基线改善了回报-OOD权衡。理论分析确立了水平α校准、简洁稳定性界限和回报比较。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略添加了原则性、校准的风险控制，特别在支持外误差占主导时优于Q引导。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [10] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应分割状态轨迹来发现选项，解决了分层强化学习中自主发现语义子目标和学习最优选项终止边界的挑战。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在长时程任务中通过引入时间抽象来增强决策的可扩展性，但实际应用中面临自主发现语义子目标和学习最优选项终止边界的挑战。

Method: 集成自监督Transformer变化点检测模块到Option-Critic框架，使用启发式伪标签训练CPD模块推断环境动态的潜在变化，利用推断的变化点：(i)稳定终止函数梯度，(ii)通过分段行为克隆预训练内部选项策略，(iii)在CPD定义的状态分区上通过选项间差异惩罚强制功能专门化。

Result: 在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速收敛、更高的累积回报和显著改进的选项专门化。

Conclusion: 通过变化点分割整合结构先验可以在复杂环境中产生更可解释、样本高效和鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [11] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统解构了矩阵白化优化器，发现其性能优势不仅来自谱归一化，更重要的是方差自适应组件。实验表明方差自适应版本始终优于符号下降对应方法，且低秩方差估计器能有效降低内存成本。


<details>
  <summary>Details</summary>
Motivation: 研究各种近似"矩阵白化"变换的优化器，旨在分离解释性能的关键组件，理解为什么矩阵白化方法始终优于元素级方法如Adam。

Method: 系统解构矩阵白化优化器，通过实验比较不同变体，分析谱归一化和方差自适应组件的作用，并消融研究方差自适应策略。

Result: 所有矩阵白化方法都可靠地优于元素级对应方法；性能提升不能仅用准确的谱归一化解释，方差自适应是解释性能差距的关键因素；低秩方差估计器能减少内存成本而不损失性能。

Conclusion: 矩阵白化优化器的性能优势主要来自方差自适应组件，而非仅谱归一化；低秩方差估计器提供了有效的内存优化方案。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [12] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 该研究系统评估了基于放射组学的机器学习模型在MRI序列分布偏移下的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下保持高准确率，而使用所有特征的模型性能下降40%。数据增强显著改善了不确定性估计质量。


<details>
  <summary>Details</summary>
Motivation: 放射组学机器学习模型在临床决策支持中具有潜力，但容易受到成像协议、定位和分割变化引起的分布偏移影响，需要系统评估其鲁棒性。

Method: 使用16个水果的体模，评估了：(1)5种MRI序列的协议变化；(2)分割变化（完整、部分、旋转）；(3)观察者间变异性。训练XGBoost分类器，比较协议不变特征与序列特定特征。

Result: 使用协议不变特征训练的模型在分布偏移下F1分数>0.85，而使用所有特征的模型在协议变化下性能下降40%。数据增强使预期校准误差降低35%，温度缩放校准效果有限。

Conclusion: 协议感知的特征选择和受控体模研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [13] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出一种基于因果效应估计任务的有向无环混合图距离度量方法，用于在存在未观测混杂的情况下评估因果发现方法的性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现领域缺乏在存在潜在混杂情况下有效评估发现图质量的指标，现有方法难以准确衡量因果发现的进展。

Method: 使用基于固定识别和符号验证器的方法，量化图差异如何扭曲不同处理-结果对的因果效应估计量。

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较。

Conclusion: 提出的图距离度量为在未观测混杂情况下评估因果发现方法提供了新的评估框架。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [14] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为DWMGrad的新型优化算法，通过动态指导机制动态更新动量和学习率，解决了传统优化算法在处理复杂模型和非凸优化问题时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习研究中，随机梯度下降（SGD）和自适应矩估计（Adam）等优化算法在处理学习效率波动、复杂模型需求和非凸优化问题时存在明显不足，主要源于算法在处理复杂数据结构和模型时的局限性。

Method: 基于传统方法基础，引入依赖历史数据的动态指导机制，动态更新动量和学习率，使优化器能灵活调整对历史信息的依赖，适应不同的训练场景。

Result: 通过大量实验验证，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法能够更好地适应变化的环境和任务复杂性，在深度学习优化方面表现出优越性能。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [15] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐系统中LoRA持续学习的新方法，通过近端正则化来平衡模型适应性和稳定性，相比现有方法能更好地捕捉用户最新行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的持续学习方法主要关注保持对过去任务的性能，但忽略了推荐系统的特殊性：目标不是预测过去的偏好，过时的偏好甚至会在用户兴趣显著变化时损害性能。

Method: 提出PESO方法，引入近端正则化器，将当前适配器锚定到其最近的冻结状态，使模型能够灵活平衡适应性和保持性，更好地捕捉最近的用户行为。

Result: 理论分析表明这种近端设计在LoRA子空间中提供了数据感知、方向性指导。实证结果显示PESO在持续学习性能上一致优于现有基于LoRA的方法。

Conclusion: PESO通过近端正则化有效解决了推荐系统中持续学习的挑战，能够更好地适应不断变化的用户偏好。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [16] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文探讨了在训练和测试分布比例不同的情况下，分布偏移可能带来性能提升的现象。研究发现，在某些设置中，训练和测试比例不匹配反而能提高测试性能，即使各组件之间没有相关性或迁移学习。


<details>
  <summary>Details</summary>
Motivation: 研究分布偏移对模型性能的影响，特别是当训练和测试数据的混合分布比例不同时，这种不匹配是否可能带来性能提升。

Method: 通过分析混合分布场景，识别最优训练比例，并在多种情境下研究分布偏移的益处，包括组件技能分布不同的组合设置。

Result: 研究表明，在某些情况下分布偏移确实是有益的，测试性能可以因训练比例不匹配而提高，即使各组件之间没有相关性。

Conclusion: 分布偏移在特定条件下可以带来性能提升，本文识别了最优训练比例，并展示了这种分布偏移益处的程度。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [17] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 本文提出了一个统一的双层对抗学习模型，研究了聚类模型中的对抗攻击机制，从数据扰动角度解释攻击原理，并分析了δ-度量的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型结构复杂，对抗攻击机制尚未得到很好解释，攻击效果测量方法也不明确，需要建立统一的对抗学习框架。

Method: 提出统一的双层对抗学习模型，从数据扰动角度研究聚类模型的对抗攻击机制，分析δ-度量的良好定义性。

Result: 发现当数据扰动较小时聚类模型具有鲁棒性，当扰动较大时聚类结果发生变化导致攻击，δ-度量可用于双层模型中的对抗学习。

Conclusion: 建立了对抗学习的统一框架，揭示了聚类模型对抗攻击的数据扰动机制，验证了δ-度量在测量攻击效果方面的有效性。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [18] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，将数据增强技术与因果推断相结合，论证了数据增强在干预泛化中的应用价值，并引入了IV-like回归概念来缓解混杂偏倚。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强主要用于i.i.d.设置下的正则化，但在存在未观测混杂的情况下，工具变量往往难以获得。本文旨在探索数据增强在因果效应估计和跨干预泛化中的潜力。

Method: 将参数化数据增强建模为IV-like回归问题，通过适当正则化工具变量估计器来缓解混杂偏倚，并利用数据增强组合模拟最坏情况应用。

Result: 理论分析和模拟实验表明，该方法在因果估计和泛化任务上优于简单数据增强，在线性示例中验证了有限样本情况下的有效性，并通过真实数据实验支持了论点。

Conclusion: 数据增强可以超越传统i.i.d.设置，在因果推断中作为干预工具，通过IV-like回归框架有效缓解混杂偏倚，提升跨干预的泛化性能。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [19] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种Lipschitz感知的线性嫁接方法，通过将非线性激活函数替换为线性函数来消除近似误差，从而获得更紧的局部Lipschitz常数，提高认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在认证鲁棒性中，较小的Lipschitz常数意味着模型对其预测具有更强的对抗样本鲁棒性。然而，近似误差阻碍了获得紧的局部Lipschitz常数，这是认证鲁棒性的关键。

Method: 提出Lipschitz感知的线性嫁接方法，将非线性激活函数替换为线性函数以消除近似误差，因为线性函数不需要松弛。

Result: 实验表明，将线性性嫁接到有影响力的激活函数中可以收紧l∞局部Lipschitz常数并增强认证鲁棒性。

Conclusion: 线性嫁接通过消除主导的近似误差来收紧局部Lipschitz常数，从而改善认证鲁棒性，即使没有经过认证训练。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [20] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习引导的框架，用于快速解决最优停电问题，通过利用跨实例的共享模式，在管理野火点火风险的同时减少负荷削减。


<details>
  <summary>Details</summary>
Motivation: 为了缓解野火点火风险，电力公司需要在高风险区域对输电线路进行断电。最优停电问题是计算复杂的混合整数线性规划问题，需要在操作环境中快速频繁求解。由于特定电力系统的OPS实例具有共同结构但参数不同，这促使使用机器学习来利用跨实例的共享模式。

Method: 开发了一个机器学习引导的框架，通过扩展现有的ML引导MILP求解方法，并整合关于通电和断电线路数量的领域知识，快速生成高质量的断电决策。

Result: 在基于加利福尼亚的大规模现实合成测试系统上的结果显示，所提出的ML引导方法比传统优化方法更快地产生高质量解决方案。

Conclusion: 机器学习引导的方法能够有效解决最优停电问题，在保证解质量的同时显著提高求解速度，为电力系统野火风险管理提供了高效的工具。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [21] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于深度时间序列预测的选择性学习策略，通过双掩码机制筛选时间步来避免过拟合，显著提升了多种先进模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在时间序列预测中容易因噪声和异常值导致严重过拟合，传统方法对所有时间步进行统一优化会学习不确定和异常的时间步，最终造成过拟合问题。

Method: 采用选择性学习策略，通过双掩码机制筛选时间步子集计算MSE损失：(1)不确定性掩码利用残差熵过滤不确定时间步；(2)异常掩码使用残差下界估计排除异常时间步。

Result: 在8个真实世界数据集上的实验表明，选择性学习显著提升了主流深度模型的预测性能，包括Informer的MSE降低37.4%、TimesNet降低8.4%、iTransformer降低6.5%。

Conclusion: 选择性学习策略能有效引导模型关注可泛化的时间步，忽略不可泛化的时间步，从而显著改善深度时间序列预测模型的性能并缓解过拟合问题。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [22] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于自适应损失加权的成本敏感多类正-无标签学习方法，通过为正向和推断负向损失分量分配不同的数据依赖权重，实现无偏风险估计，并在多个数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 在多类正-无标签学习场景中，由于缺乏可靠的负样本标注，许多现有方法无法保证无偏风险估计，这限制了算法的性能和稳定性。

Method: 在经验风险最小化框架下，为正向和推断负向损失分量分配不同的数据依赖权重，使经验目标成为目标风险的无偏估计器，并建立了泛化误差界。

Result: 在八个公共数据集上的广泛实验表明，该方法在不同类别先验和类别数量下，在准确性和稳定性方面均优于强基线方法。

Conclusion: 提出的自适应损失加权方法能够有效解决多类正-无标签学习中的无偏风险估计问题，提供了一致且稳定的性能提升。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [23] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 本文提出了Bulk-Space-Filtration-Accelerator (BSFA)框架，通过差异化缩放不同子空间的更新分量来加速深度学习训练，在Dom空间稳定更新，在Bulk空间加速收敛。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化中存在基本二分法：损失Hessian矩阵顶部特征方向（Dom空间）的更新幅度大但对损失减少贡献小，而正交分量（Bulk空间）的更新幅度小但驱动大部分学习进展。

Method: BSFA框架通过PCA对历史更新进行高效子空间估计，采用逐块策略在参数块基础上应用估计，差异化缩放不同子空间的更新分量。

Result: 在LLaMA-72M和LLaMA-134M预训练任务中，相比标准AdamW实现了约2倍的加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，能有效加速深度学习训练过程。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [24] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于信息瓶颈原理的归一化方法IBNorm，通过有界压缩操作在保持预测信息的同时抑制冗余变异性，相比传统的方差中心化归一化方法在理论和实验上都表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的归一化方法如BatchNorm、LayerNorm和RMSNorm都是方差中心化的，只关注零均值和单位方差来稳定训练，但没有控制表示如何捕获任务相关信息。

Method: 基于信息瓶颈原理，提出IBNorm方法，引入有界压缩操作，鼓励嵌入保持预测信息同时抑制冗余变异性。

Result: 理论上证明IBNorm比方差中心化方法获得更高的IB值和更紧的泛化边界；实验上在大型语言模型（LLaMA、GPT-2）和视觉模型（ResNet、ViT）中一致优于BatchNorm、LayerNorm和RMSNorm，互信息分析确认了更好的信息瓶颈行为。

Conclusion: IBNorm是一种简单而强大的归一化方法系列，能够产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [25] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律发现。该框架采用两级架构，第一级学习PDE的基本符号组件，第二级学习它们的控制组合，实现了先验物理知识的结构化集成。


<details>
  <summary>Details</summary>
Motivation: 建模复杂时空动态，特别是远离平衡系统的动态，是科学中的重大挑战。这些系统的控制偏微分方程由于高阶导数和强非线性等固有复杂性，难以从第一原理推导，且物理知识不完整。现有数据驱动方法存在物理不一致性和数据密集性问题，而现有物理信息方法缺乏表示复杂算子或系统集成部分物理知识的结构能力。

Method: 提出分层物理嵌入学习框架，采用两级架构：第一级学习PDE的基本符号组件，第二级学习它们的控制组合。该框架基于自适应傅里叶神经算子构建，能够有效捕捉动态系统的非局部依赖和高阶算子特征。通过结构化解耦已知和未知项，实现可解释的底层控制方程发现。

Result: 该框架能够保证物理一致性并提高数据效率，通过将已知物理定律直接嵌入模型计算图中，实现了先验知识的结构化集成。同时能够从稀疏噪声数据中进行可解释的物理定律发现，无需预设函数形式。

Conclusion: 分层物理嵌入学习框架在时空预测和物理定律发现方面取得了根本性进展，通过层次分解不仅降低了学习复杂性，更重要的是实现了先验物理知识的结构化集成，为复杂系统的建模提供了新的有效途径。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [26] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 本文提出了一种多目标强化学习算法，旨在学习既能最大化期望回报，又能在目标状态上实现分散边际状态分布的策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法主要关注最大化期望回报，但可能导致策略过度利用少数奖励源。在自然场景中，需要学习能在目标状态上产生分散边际状态分布的策略，同时保持高回报。现有方法如熵正则化和内在奖励主要鼓励探索以找到最优策略，但不一定能实现目标状态的分散分布。

Method: 提出了一种新颖算法，通过优化自定义的强化学习奖励来学习高回报策略混合体。该奖励基于当前策略混合体在每次迭代中为采样轨迹计算，然后使用离线强化学习算法更新策略混合体。

Result: 算法在合成MDP和标准RL环境中进行了实验验证，证明了其有效性。

Conclusion: 本文提出的算法能够有效地在最大化期望回报的同时，实现目标状态上边际状态分布的分散，解决了多目标强化学习中的关键问题。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [27] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出了一种基于循环矩阵和对角矩阵乘积的新型可逆线性层，显著降低了参数复杂度和计算复杂度，并在此基础上构建了CDFlow模型，在自然图像数据集上取得了良好的密度估计效果。


<details>
  <summary>Details</summary>
Motivation: 设计能够增强表达能力同时保持雅可比行列式和逆矩阵高效计算的可逆线性层，是标准化流模型面临的关键挑战。

Method: 引入基于循环矩阵和对角矩阵乘积分解的新型可逆线性层，利用快速傅里叶变换降低计算复杂度，并构建Circulant-Diagonal Flow (CDFlow)模型。

Result: 将参数复杂度从O(n²)降低到O(mn)，矩阵求逆时间复杂度从O(n³)降低到O(mn log n)，对数行列式计算从O(n³)降低到O(mn)，在自然图像数据集上实现了强大的密度估计。

Conclusion: CDFlow不仅有效建模具有周期性结构的数据，还显著加速了标准化流中的关键操作，为可扩展生成建模提供了实际优势。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [28] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 该论文提出了信息级联流行度预测的新方法，解决了现有方法的三个关键问题：时间泄漏、特征贫乏数据集和计算效率低下。通过时间有序分割策略、大规模电商数据集Taoke和轻量级框架CasTemp，实现了无泄漏评估下的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前信息级联流行度预测中的三个关键限制：时间泄漏问题导致模型访问未来信息、特征贫乏数据集缺乏下游转化信号、以及复杂图方法计算效率低下。

Method: 1. 时间有序分割策略：按时间顺序将数据分割为连续窗口，避免未来信息泄漏；2. 构建Taoke数据集：包含丰富的推广者/产品属性和真实购买转化；3. 开发CasTemp框架：使用时间游走、基于Jaccard的邻居选择和GRU编码的时间感知注意力机制。

Result: 在无泄漏评估下，CasTemp在四个数据集上实现了最先进的性能，计算速度提升了几个数量级。特别擅长预测第二阶段的流行度转化，这对实际应用至关重要。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个方面的挑战，提出的方法在信息级联流行度预测中实现了更现实、高效和实用的解决方案。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [29] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文对随机几何超图上的变分学习进行了渐近一致性分析，提出了高阶超图学习方法，该方法在标准基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 超图为建模高阶交互提供了自然框架，但在半监督学习中的理论基础仍然有限。

Method: 提出了高阶超图学习方法，通过骨架图的拉普拉斯算子幂进行多尺度平滑正则化。

Result: 该方法收敛到高阶Sobolev半范数，在标准基准测试中表现强劲。

Conclusion: 超图学习具有良好的理论基础和实际性能，为高阶交互建模提供了有效工具。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [30] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 本文提出在知识图谱嵌入模型中使用模型合并方法（特别是加权平均）来替代传统的集成学习方法，通过维护训练过程中模型参数的运行平均值来提升链接预测性能，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法需要训练多个模型，导致计算开销大、延迟高和内存占用多。模型合并方法提供了有前景的替代方案，无需训练多个模型即可获得集成学习的好处。

Method: 提出两种加权平均方法：1）从训练周期开始维护模型参数的运行平均值；2）选择性更新方法，仅在验证集上泛化性能提升时才更新集成模型参数的运行平均值。

Result: 在链接预测任务上，提出的加权平均方法相比最先进的基准集成方法表现更好，在字面增强KGE模型和多跳查询回答任务中也持续提升性能。

Conclusion: 加权平均方法在多种评估设置中都能持续改进性能，为知识图谱嵌入模型提供了一种高效且有效的集成学习替代方案。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [31] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 本文提出了一种基于损失函数从初始非凸性向最优解附近凸性转换假设的两阶段优化算法，通过检测转换点分别使用非凸方法（Adam）和凸方法（共轭梯度）来提升收敛性和精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习中损失函数常包含非凸区域，导致广泛使用非凸优化方法如Adam。但局部最小值周围环境通常是凸的，此时二阶方法能保证超线性收敛。本文假设实际任务中损失函数会从初始非凸性向最优解附近的凸性转换，并利用这一特性设计优化算法。

Method: 提出两阶段优化框架：1）检测损失函数从非凸到凸的转换点，通过观察梯度范数与损失的关系；2）在非凸区域使用Adam算法，在凸区域使用共轭梯度法。

Result: 计算实验证实了该假设的实用性，表明这种简单的凸性结构在实际中足够常见，能够被有效利用来显著改善收敛性和精度。

Conclusion: 利用损失函数从非凸到凸的转换特性设计的混合优化算法在实践中能够有效提升性能，验证了所提出假设的合理性。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [32] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大语言模型的优化方法，通过在大规模合成数据集上微调LLM，使其具备连续黑盒优化能力，无需参数调优即可超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法需要针对每个应用领域进行参数调优，而现有大语言模型在连续黑盒优化任务上能力有限，需要开发更通用的优化方法。

Method: 通过从多样化BO参数化生成的大规模合成数据集上微调大语言模型，利用LLM预训练实现跨优化任务的泛化能力。

Result: 在各种黑盒优化基准测试中，GPTOpt超越了传统优化器，展示了LLM在高级数值推理方面的能力。

Conclusion: GPTOpt为无需参数调优的全局优化提供了一个灵活框架，证明了LLM在连续黑盒优化任务上的潜力。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [33] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 该论文提出了一个名为效用校准的通用框架，用于评估多分类器的校准性能，该框架通过特定效用函数来衡量校准误差，能够统一和重新解释现有的校准指标。


<details>
  <summary>Details</summary>
Motivation: 确保分类器良好校准（预测与观测频率一致）是分类器可信赖的基本要求。现有方法要么关注预测的特定方面，要么使用计算复杂的变分公式，因此需要可扩展的多分类校准评估方法。

Method: 提出效用校准框架，通过特定效用函数来衡量校准误差，该效用函数封装了最终用户相关的目标或决策标准。该框架可以统一和重新解释现有的校准指标。

Result: 该框架能够生成更稳健的顶级分类和类级校准指标版本，并超越二值化方法，评估更丰富的下游效用类别的校准性能。

Conclusion: 效用校准提供了一个通用且可扩展的框架来评估多分类器的校准性能，能够适应不同的用户需求和决策场景。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [34] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 本文提出梯度权重对齐（GWA）指标，通过量化每个样本梯度与模型权重之间的一致性来监控深度学习训练过程，无需验证集即可预测最佳早停点、比较模型性能并识别有影响力的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在监督分类任务中，需要一种能够检测过拟合、监控训练动态并归因性能到单个训练样本的鲁棒验证指标，而现有方法往往依赖验证集。

Method: 引入梯度权重对齐（GWA）指标，计算每个样本梯度与模型权重之间的相干性，有效学习对应一致的对齐，而错位表示泛化能力下降。

Result: 大量实验表明，GWA能够准确预测最佳早停点，支持有原则的模型比较，并识别有影响力的训练样本。

Conclusion: GWA提供了一种无需验证集的方法，直接从训练数据中分析模型，为深度学习训练监控和模型分析提供了新的工具。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [35] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 本文提出原型神经符号架构，通过原型学习理论解决神经符号AI中的推理捷径问题，确保模型基于正确概念而非伪相关进行推理，在极低数据量下仍能有效学习正确概念。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI模型容易学习到推理捷径，即利用伪相关性而非正确概念来满足符号约束，这影响了模型的可靠性和安全性。

Method: 引入原型神经符号架构，基于原型学习理论训练模型，在满足背景知识的同时考虑输入与少量标注数据的相似性，从而避免推理捷径。

Result: 在rsbench基准测试中，无论是合成任务（MNIST-EvenOdd和Kand-Logic）还是高风险现实任务（BDD-OIA），该方法在极低监督下显著改善了正确概念的学习效果。

Conclusion: 原型基础化为实现安全可靠的神经符号学习提供了一种有效且标注效率高的策略，为解决推理捷径问题开辟了新途径。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [36] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法预测员工倦怠风险，在KNN、随机森林和SVM三种算法中，SVM表现最佳（R²=0.84），并开发了Streamlit交互界面供非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 倦怠综合征严重影响个人福祉和组织绩效，需要早期检测和干预。

Method: 使用HackerEarth员工倦怠挑战数据集，评估KNN、随机森林和SVM三种监督学习算法，通过30折交叉验证和R²指标评估性能。

Result: SVM模型预测性能最佳（R²=0.84），在配对t检验中显著优于KNN和随机森林。开发了Streamlit交互界面实现实际应用。

Conclusion: 机器学习在组织环境中支持倦怠早期检测和基于数据的心理健康策略方面具有潜力。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [37] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了FP和INT量化格式在不同粒度下的性能，发现FP在粗粒度量化中表现优异，但在细粒度（块级）量化中，MXINT8在算法精度和硬件效率上均优于FP格式。对于4位格式，FP通常具有精度优势，但通过异常值缓解技术，NVINT4可以超越NVFP4。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件越来越多地采用低精度浮点格式处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式之间的权衡，包括不同粒度（粗粒度和细粒度）和位宽（8位和4位）的比较，并引入对称裁剪方法解决细粒度低位INT训练中的梯度偏差问题。

Result: 发现性能临界点：FP在粗粒度量化中表现优异，但在细粒度量化中，MXINT8在算法精度和硬件效率上均优于FP格式；对于4位格式，FP通常具有精度优势，但NVINT4通过Hadamard旋转等异常值缓解技术可以超越NVFP4。

Conclusion: 挑战当前硬件发展趋势，证明一刀切的FP方法不是最优选择，建议细粒度INT格式（特别是MXINT8）为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [38] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是对WGAN框架的简单但有效改进，受贝叶斯最优学习阈值启发。它使用Lipschitz连续判别器，隐式最小化不同于Wasserstein距离的度量距离，在四个标准图像生成基准测试中比WGAN表现更好，FID降低10-60%。


<details>
  <summary>Details</summary>
Motivation: 改进WGAN框架的训练稳定性，通过贝叶斯最优学习阈值原理来增强GAN训练效果。

Method: 在WGAN框架基础上引入BOLT原理，使用Lipschitz连续判别器，隐式最小化不同于Wasserstein距离的度量距离。

Result: 在CIFAR-10、CelebA-64、LSUN Bedroom-64和LSUN Church-64四个基准测试中，BOLT-GAN比WGAN表现更好，FID降低10-60%，训练稳定性更好。

Conclusion: BOLT是一个广泛适用于增强GAN训练效果的原则，BOLT-GAN在多个图像生成任务中表现出色。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [39] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了一种基于核引导互信息（KG-MI）的新信息论度量方法，结合多头注意力框架，能够有效学习有向无环图（DAG）中的多父节点依赖关系，并证明了在多项式时间内收敛到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的图结构学习方法主要局限于树状图结构，难以扩展到具有多父节点的通用有向无环图（DAG），主要挑战在于设计训练目标使不同注意力头能够分别学习多个不同的父节点关系。

Method: 引入基于f-散度的核引导互信息（KG-MI）度量，结合多头注意力框架，每个注意力头关联不同的边际转移核来有效建模多样化的父子依赖关系。

Result: 证明了对于K-父节点DAG生成的序列，通过梯度上升训练单层多头transformer能够在多项式时间内收敛到全局最优解，并刻画了收敛时的注意力得分模式。当f-散度特化为KL散度时，学习到的注意力得分准确反映了真实邻接矩阵，从而可证明地恢复底层图结构。

Conclusion: 实验验证了理论发现，该方法能够有效恢复隐藏的图结构，为理解transformer在图结构学习中的训练动态提供了理论保证。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [40] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文系统研究了视觉-语言-动作(VLA)模型在动作微调过程中视觉-语言(VL)表示保留问题，发现朴素的动作微调会导致视觉表示退化，并提出了一种简单有效的对齐方法来缓解退化并改善OOD泛化。


<details>
  <summary>Details</summary>
Motivation: 研究VLA模型在动作微调过程中，预训练的视觉-语言模型(VLMs)的原始视觉-语言表示和知识保留程度，以理解动作微调与VL能力退化之间的权衡。

Method: 通过探测VLA隐藏表示和分析注意力图来表征和测量表示退化；设计对比任务和方法来隔离动作微调引起的VL能力变化；评估多种视觉表示对齐策略，并提出简单有效的对齐方法。

Result: 发现朴素动作微调会导致视觉表示退化；提出的对齐方法能够有效缓解退化，并在分布外(OOD)场景中实现更好的泛化性能。

Conclusion: 阐明了动作微调与VL表示退化之间的权衡关系，并提出了实用的方法来恢复继承的VL能力，为VLA模型的优化提供了重要指导。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [41] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedLap是一个用于图结构数据联邦学习的新框架，通过拉普拉斯平滑在谱域中利用全局结构信息，有效捕捉节点间依赖关系，同时确保隐私和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中图结构数据分布在不同客户端的问题，特别是互联子图场景，现有方法存在隐私风险或计算复杂度高的限制。

Method: 使用拉普拉斯平滑在谱域中捕获全局结构信息，避免交换敏感节点嵌入，提供形式化隐私分析。

Result: 在基准数据集上的广泛实验表明，FedLap相比现有技术实现了竞争性或更优的性能。

Conclusion: FedLap是第一个具有强隐私保证的子图联邦学习方案，在保持隐私的同时实现了良好的效用。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [42] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，从随机PAC-Bayesian保证中提取适用于单个假设的泛化保证，解决了经典PAC-Bayes只能提供随机采样假设期望风险保证的问题。


<details>
  <summary>Details</summary>
Motivation: 经典PAC-Bayes框架只能提供随机采样假设的期望风险保证，这需要测试时的随机预测，使得PAC-Bayes在许多需要部署单个确定性假设的实际场景中无法使用。

Method: 提出了一个统一框架，包括一个通用oracle边界，从中推导出数值边界和专门针对多数投票的特殊化方法。

Result: 实证研究表明，该方法在确定性分类器的泛化边界方面始终优于流行基线（最多可提高2倍）。

Conclusion: 该框架成功地将随机PAC-Bayesian保证转化为适用于单个确定性假设的泛化保证，显著提升了PAC-Bayes在实际应用中的实用性。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [43] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 该论文研究了在噪声环境下低秩伪逆矩阵的谱范数鲁棒性，推导了尖锐的非渐近扰动界，揭示了误差如何随特征间隙、谱衰减和噪声对齐而缩放。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵常带有噪声，但低秩逆近似的谱范数鲁棒性仍缺乏系统理解，需要为噪声计算环境提供实用的谱感知保证。

Method: 引入轮廓积分技术的新颖应用，针对非全纯函数f(z)=1/z，在温和噪声假设下推导扰动界，改进经典全逆界达√n倍。

Result: 推导的边界在各种真实和合成矩阵上紧密跟踪真实扰动误差，而基于经典结果的估计往往显著高估误差。

Conclusion: 研究为噪声计算环境中的低秩逆近似提供了实用的谱感知保证，改进经典方法并更准确预测实际误差。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [44] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Orlicz几何结构的广义Sobolev IPM（GSI）方法，通过Musielak正则化将复杂优化问题简化为单变量优化，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有Sobolev IPM方法受限于L^p几何结构，无法融入其他结构先验，需要一种更灵活的几何结构框架。

Method: 利用Orlicz几何结构推广Sobolev IPM，建立Orlicz-Sobolev范数与Musielak范数的理论联系，并提出Musielak正则化将问题简化为单变量优化。

Result: GSI-M方法比流行的OW方法快几个数量级，在文档分类和拓扑数据分析任务中表现出优越性能。

Conclusion: 提出的GSI-M框架不仅扩展了Sobolev IPM的几何结构适用范围，还通过理论创新解决了计算效率问题，具有实际应用价值。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [45] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一个基于适当评分规则（特别是核评分）的不确定性度量框架，用于回归任务中的总不确定性、偶然不确定性和认知不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 当前文献主要关注分类任务的不确定性量化，而安全关键领域的回归任务同样需要适当的不确定性度量。

Method: 基于适当评分规则（特别是核评分）构建不确定性度量框架，统一了多个已知度量方法，并通过核选择来控制度量的行为特性（如尾部敏感性、鲁棒性和分布外响应性）。

Result: 实验表明这些度量在下游任务中有效，并揭示了不同实例化之间的权衡关系，包括鲁棒性和分布外检测性能。

Conclusion: 该框架为任务特定的不确定性度量设计提供了原则性指导，证明了核评分特性与下游行为之间的明确对应关系。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [46] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 本文建立了对称矩阵谱范数扰动的新边界，改进了经典的Eckart-Young-Mirsky定理，并应用于差分隐私PCA，解决了文献中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习中需要理解噪声或测量误差如何影响低秩近似，特别是在谱范数下。差分隐私低秩近似中，需要在保护隐私的同时保持数据的top-p结构。现有工作主要分析Frobenius范数误差或重建质量变化，但这些指标可能高估或低估真实的子空间失真。

Method: 使用复分析中的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱函数类。在温和的特征值间隙和范数条件下，建立了对称矩阵的高概率谱范数扰动边界。

Result: 新边界对‖(A+E)_p - A_p‖给出了尖锐估计，改进因子可达√n。经验结果证实，新边界在不同扰动机制下紧密跟踪实际谱误差。

Conclusion: 本文的谱范数扰动边界为差分隐私PCA提供了改进的效用保证，解决了文献中的开放问题，并为广泛的谱函数分析提供了新工具。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [47] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [48] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出卷积脉冲GRU（CS-GRU）单元，结合卷积操作保留局部结构，集成脉冲神经元的时间精度和GRU的高效门控机制，在时序和时空数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时丢失局部细节，现有方法如SpikGRU无法捕捉事件型时空数据中的细粒度局部依赖关系。

Method: 引入卷积脉冲GRU（CS-GRU）单元，利用卷积操作保留局部结构和依赖关系，同时整合脉冲神经元的时间精度和GRU的门控机制。

Result: CS-GRU在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，比SpikGRU效率高69%。

Conclusion: CS-GRU是一个多功能架构，在时序和时空数据处理任务中表现出色，为事件型数据提供了高效的解决方案。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [49] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 本文分析了传统多实例学习（MIL）方法在处理医学图像时的局限性，即忽略了相邻实例间的上下文关系。作者设计了一个合成分类任务来验证这一局限，并比较了现有MIL方法与最优贝叶斯估计器的性能差距。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在处理医学图像时，将每个实例（如切片或补丁）独立处理，忽略了相邻实例间的上下文关系，而这些关系在实际应用中可能至关重要。

Method: 设计了一个合成分类任务，其中相邻实例的特征对准确预测至关重要。通过量化现有MIL方法与最优贝叶斯估计器的性能差距来验证方法的局限性。

Result: 实证研究表明，即使是最新的相关MIL方法，在从数万个实例从头开始训练时，仍然难以达到最优的泛化性能。

Conclusion: 当前MIL方法在处理需要利用相邻实例上下文关系的任务时存在明显局限，需要开发更有效的方法来捕捉这种相关性。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [50] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了神经随机流（NSFs）及其潜在变体，通过条件归一化流直接学习随机微分方程的转移规律，实现任意时间点的一次性采样，相比传统数值方法可获得两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统随机微分方程建模方法需要昂贵的数值求解器在任意时间点之间进行采样，计算成本高。

Method: 使用具有架构约束的条件归一化流来学习（潜在）随机微分方程的转移规律，这些约束保留了随机流继承的特性。

Result: 在合成随机微分方程模拟和真实世界跟踪及视频数据上的实验表明，NSFs在保持与数值方法相当的分布精度的同时，大幅减少了任意时间点采样的计算量。

Conclusion: 神经随机流提供了一种高效的方法来建模噪声和不规则采样的时间序列，在金融、物理和机器学习领域具有应用潜力。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [51] [RNAGenScape: Property-guided Optimization and Interpolation of mRNA Sequences with Manifold Langevin Dynamics](https://arxiv.org/abs/2510.24736)
*Danqi Liao,Chen Liu,Xingzhi Sun,Dié Tang,Haochen Wang,Scott Youlten,Srikar Krishna Gopinath,Haejeong Lee,Ethan C. Strayer,Antonio J. Giraldez,Smita Krishnaswamy*

Main category: q-bio.QM

TL;DR: RNAGenScape是一个基于流形Langevin动力学的mRNA设计框架，通过属性引导的潜在空间探索来优化mRNA序列，在数据稀缺情况下仍能有效提升目标属性。


<details>
  <summary>Details</summary>
Motivation: mRNA设计和优化在合成生物学和治疗开发中很重要，但机器学习研究不足。系统优化受到数据稀缺、不平衡以及复杂序列-功能关系的阻碍。

Method: 结合组织化自编码器（结构化潜在空间）和流形投影器（将更新步骤收缩回流形），使用属性引导的流形Langevin动力学迭代更新mRNA序列。

Result: 在三个真实mRNA数据集上，RNAGenScape以高成功率和效率改善目标属性，优于为蛋白质或非生物数据开发的各种生成或优化方法。

Conclusion: RNAGenScape通过提供连续、数据对齐的轨迹来揭示编辑如何影响功能，为可控mRNA设计和mRNA序列建模中的潜在空间探索建立了可扩展范例。

Abstract: mRNA design and optimization are important in synthetic biology and
therapeutic development, but remain understudied in machine learning.
Systematic optimization of mRNAs is hindered by the scarce and imbalanced data
as well as complex sequence-function relationships. We present RNAGenScape, a
property-guided manifold Langevin dynamics framework that iteratively updates
mRNA sequences within a learned latent manifold. RNAGenScape combines an
organized autoencoder, which structures the latent space by target properties
for efficient and biologically plausible exploration, with a manifold projector
that contracts each step of update back to the manifold. RNAGenScape supports
property-guided optimization and smooth interpolation between sequences, while
remaining robust under scarce and undersampled data, and ensuring that
intermediate products are close to the viable mRNA manifold. Across three real
mRNA datasets, RNAGenScape improves the target properties with high success
rates and efficiency, outperforming various generative or optimization methods
developed for proteins or non-biological data. By providing continuous,
data-aligned trajectories that reveal how edits influence function, RNAGenScape
establishes a scalable paradigm for controllable mRNA design and latent space
exploration in mRNA sequence modeling.

</details>


### [52] [CT-Less Attenuation Correction Using Multiview Ensemble Conditional Diffusion Model on High-Resolution Uncorrected PET Images](https://arxiv.org/abs/2510.24805)
*Alexandre St-Georges,Gabriel Richard,Maxime Toussaint,Christian Thibaudeau,Etienne Auger,Étienne Croteau,Stephen Cunnane,Roger Lecomte,Jean-Baptiste Michaud*

Main category: q-bio.QM

TL;DR: 该论文提出使用条件去噪扩散概率模型从非衰减校正PET图像生成高质量CT图像，以解决PET成像中的衰减问题，避免传统CT方法带来的额外辐射暴露和配准问题。


<details>
  <summary>Details</summary>
Motivation: PET成像中的衰减问题会导致定量不准确，传统使用CT进行衰减校正的方法存在额外辐射暴露、空间配准问题和设备成本高的缺点。

Method: 采用条件去噪扩散概率模型，结合三个正交视图的非衰减校正PET图像，通过集成投票生成伪CT图像。

Result: 在159个头扫描数据上的研究表明，该方法生成的伪CT图像质量更高，伪影减少，切片间一致性改善，CT图像的平均绝对误差为32±10.4 HU，PET重建的平均误差为(1.48±0.68)%。

Conclusion: 基于扩散模型的伪CT生成方法能够有效解决PET衰减校正问题，减少对传统CT的依赖，降低辐射暴露和成本。

Abstract: Accurate quantification in positron emission tomography (PET) is essential
for accurate diagnostic results and effective treatment tracking. A major issue
encountered in PET imaging is attenuation. Attenuation refers to the diminution
of photon detected as they traverse biological tissues before reaching
detectors. When such corrections are absent or inadequate, this signal
degradation can introduce inaccurate quantification, making it difficult to
differentiate benign from malignant conditions, and can potentially lead to
misdiagnosis. Typically, this correction is done with co-computed Computed
Tomography (CT) imaging to obtain structural data for calculating photon
attenuation across the body. However, this methodology subjects patients to
extra ionizing radiation exposure, suffers from potential spatial
misregistration between PET/CT imaging sequences, and demands costly equipment
infrastructure. Emerging advances in neural network architectures present an
alternative approach via synthetic CT image synthesis. Our investigation
reveals that Conditional Denoising Diffusion Probabilistic Models (DDPMs) can
generate high quality CT images from non attenuation corrected PET images in
order to correct attenuation. By utilizing all three orthogonal views from
non-attenuation-corrected PET images, the DDPM approach combined with ensemble
voting generates higher quality pseudo-CT images with reduced artifacts and
improved slice-to-slice consistency. Results from a study of 159 head scans
acquired with the Siemens Biograph Vision PET/CT scanner demonstrate both
qualitative and quantitative improvements in pseudo-CT generation. The method
achieved a mean absolute error of 32 $\pm$ 10.4 HU on the CT images and an
average error of (1.48 $\pm$ 0.68)\% across all regions of interest when
comparing PET images reconstructed using the attenuation map of the generated
pseudo-CT versus the true CT.

</details>


### [53] [General Microstructure Factor Analysis of Diffusion MRI in Gray-Matter Predicts Cognitive Scores](https://arxiv.org/abs/2510.24879)
*Lucas Z. Brito,Ryan P. Cabeen,David H. Laidlaw*

Main category: q-bio.QM

TL;DR: 该研究使用扩散MRI和NODDI技术探索灰质微观结构模式，发现基于PCA的全局微观结构指标能够预测认知表现，特别是与阅读、词汇和认知流畅性相关。


<details>
  <summary>Details</summary>
Motivation: 扩散MRI在白质微观结构研究中已有重要发现，但在灰质应用方面相对较少探索，因此研究灰质微观结构的全局模式及其与认知表现的关系。

Method: 使用人类连接组计划青年成人研究的扩散MRI和行为数据，计算区域平均NODDI参数，应用主成分分析构建灰质微观结构的一般因素。

Result: 发现源自各向同性体积分数的因子解释了显著的个体间变异性，并与NIH工具箱收集的特定认知分数显著相关，特别是与阅读、词汇表现和认知流畅性相关。

Conclusion: 一般微观结构因子可作为研究群体水平认知和皮层组织的稳健、可解释的生物标志物，为结构-功能关系提供补充性标记。

Abstract: Diffusion MRI has revealed important insights into white matter
microstructure, but its application to gray matter remains comparatively less
explored. Here, we investigate whether global patterns of gray-matter
microstructure can be captured through neurite orientation dispersion and
density imaging (NODDI) and whether such patterns are predictive of cognitive
performance. Our findings demonstrate that PCA-based global indicators of
gray-matter microstructure provide complementary markers of structure-function
relationships, extending beyond region-specific analyses. Our results suggest
that general microstructure factors may serve as robust, interpretable
biomarkers for studying cognition and cortical organization at the population
level. Using diffusion MRI and behavioral data from the Human Connectome
Project Young Adult study, we derived region-averaged NODDI parameters and
applied principal component analysis (PCA) to construct general gray-matter
microstructure factors. We found that the factor derived from isotropic volume
fraction explained substantial inter-individual variability and was
significantly correlated with specific cognitive scores collected from the NIH
Toolbox. In particular, the isotropic volume fraction factor was linked to
reading and vocabulary performance and to cognitive fluidity.

</details>


### [54] [scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration](https://arxiv.org/abs/2510.24987)
*Jianle Sun,Chaoqi Liang,Ran Wei,Peng Zheng,Lei Bai,Wanli Ouyang,Hongliang Yan,Peng Ye*

Main category: q-bio.QM

TL;DR: 提出了一种名为scMRDR的可扩展生成框架，用于整合未配对的多组学单细胞数据，通过解耦模态共享和模态特定的潜在表示来解决整合挑战。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序技术发展使得能够对多种分子模态进行高分辨率分析，但整合未配对的多组学单细胞数据仍然具有挑战性。现有方法要么依赖配对信息或先验对应关系，要么需要计算全局成对耦合矩阵，限制了其可扩展性和灵活性。

Method: 使用精心设计的β-VAE架构将每个细胞的潜在表示解耦为模态共享和模态特定组件，并通过等距正则化保留组学内生物异质性、对抗目标鼓励跨模态对齐，以及掩码重建损失策略解决跨模态特征缺失问题。

Result: 在基准数据集上取得了优异的性能，包括批次校正、模态对齐和生物信号保留。关键的是，该方法能够有效扩展到大规模数据集，并支持整合两个以上的组学。

Conclusion: scMRDR为大规模多组学数据整合和下游生物学发现提供了一个强大而灵活的解决方案。

Abstract: Advances in single-cell sequencing have enabled high-resolution profiling of
diverse molecular modalities, while integrating unpaired multi-omics
single-cell data remains challenging. Existing approaches either rely on pair
information or prior correspondences, or require computing a global pairwise
coupling matrix, limiting their scalability and flexibility. In this paper, we
introduce a scalable and flexible generative framework called single-cell
Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired
multi-omics integration. Specifically, we disentangle each cell's latent
representations into modality-shared and modality-specific components using a
well-designed $\beta$-VAE architecture, which are augmented with isometric
regularization to preserve intra-omics biological heterogeneity, adversarial
objective to encourage cross-modal alignment, and masked reconstruction loss
strategy to address the issue of missing features across modalities. Our method
achieves excellent performance on benchmark datasets in terms of batch
correction, modality alignment, and biological signal preservation. Crucially,
it scales effectively to large-level datasets and supports integration of more
than two omics, offering a powerful and flexible solution for large-scale
multi-omics data integration and downstream biological discovery.

</details>


### [55] [Stress distribution in contractile cell monolayers](https://arxiv.org/abs/2510.25651)
*Yucheng Huo,Kexin Guo,Massimo Paradiso,K. Jimmy Hsia*

Main category: q-bio.QM

TL;DR: 该研究通过整合牵引力显微镜和数值模拟，重构了C2C12成肌细胞单层中的应力分布，揭示了局部机械力在决定集体细胞结构中的作用。研究发现收缩性单层表现出正最大和负最小主应力，反映了主动张力的内在各向异性。在拓扑缺陷周围出现不同的应力模式，与细胞排列、密度和形态的奇点相吻合，表明机械力与结构组织之间存在强耦合。


<details>
  <summary>Details</summary>
Motivation: 量化收缩性单层中的细胞间机械力仍然知之甚少，需要理解局部机械力在决定集体细胞结构中的作用。

Method: 整合牵引力显微镜和数值模拟，重构C2C12成肌细胞单层中的应力分布。

Result: 发现收缩性单层表现出正最大和负最小主应力，反映了主动张力的内在各向异性。在拓扑缺陷周围出现不同的应力模式，与细胞排列、密度和形态的奇点相吻合。拉伸应力优先沿细胞伸长轴传递，压缩应力横向传递，表明局部应力指导细胞排列。这种机械指导在收缩系统中具有普遍性。

Conclusion: 建立了一个定量框架来表征活性细胞单层中的机械各向异性，揭示了力-结构耦合的一般原理，为理解力学如何控制收缩性细胞系统中的肌生成、形态发生和集体组织提供了物理基础。

Abstract: Collective behaviors in cellular systems are regulated not only by
biochemical signalling pathways but also by intercellular mechanical forces,
whose quantification in contractile monolayers remains poorly understood. Here,
by integrating traction force microscopy and numerical simulations, we
reconstruct the stress distribution in C2C12 myoblast monolayers to reveal the
roles of local mechanical forces in determining the collective cellular
structures. We find that contractile monolayers exhibit positive maximum and
negative minimum principal stresses, reflecting the intrinsic anisotropy of
active tension. Distinct stress patterns emerge around topological defects,
coinciding with singularities in cell alignment, density, and morphology,
indicating a strong coupling between mechanical forces and structural
organization. Moreover, tensile stresses are preferentially transmitted along
the cell elongation axis and compressive stresses transversely, demonstrating
that local stress guides cell arrangement. This mechanical guidance appears to
be universal among contractile systems, as observed also in bone marrow-derived
mesenchymal stem cells. Together, our work establishes a quantitative framework
for characterizing mechanical anisotropy in active cellular monolayers and
reveals a general principle of force-structure coupling, providing a physical
basis for understanding how mechanics governs myogenesis, morphogenesis, and
collective organization in contractile cellular systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于推理树结构的新指标r-score来衡量查询的学习难度，并基于此开发了Re-Schedule数据调度算法，在数学推理基准测试中显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR数据调度方法通常依赖基于路径的指标来排序查询，忽视了这些查询的推理树结构，导致调度效果受限。

Method: 引入推理分数(r-score)指标来衡量查询的学习难度，基于推理树结构开发了Re-Schedule调度算法，构建从结构简单到复杂的课程学习进度。

Result: 在六个数学推理基准测试中，Re-Schedule显著提高了平均准确率，最高提升达3.2%。

Conclusion: 对推理树的结构理解为RLVR数据调度提供了更强大和原则性的基础，验证了基于推理树结构的调度方法的有效性。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [57] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究循环结构因果模型中的反事实推理，针对包含反馈环的真实系统，分析在移位-尺度干预下的反事实推断。


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理框架假设无环结构因果模型，但许多真实系统（如生物系统）包含反馈循环，违反无环性假设，需要研究循环模型中的反事实推理。

Method: 在循环结构因果模型下研究移位-尺度干预，即软性、策略式变化，对变量机制进行重新缩放和/或移位。

Result: 论文研究了循环SCMs中在移位-尺度干预下的反事实推理方法。

Conclusion: 扩展了反事实推理框架以处理包含反馈循环的循环系统，为真实世界系统的因果分析提供了更合适的建模工具。

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [58] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文提出了ProFees框架，使用LLM自动化医疗评估与管理(E/M)编码，解决了现实世界中的复杂性，在真实数据集上比商业系统准确率提高36%以上。


<details>
  <summary>Details</summary>
Motivation: 自动化E/M编码可以减轻医生的文档负担，提高计费效率，最终改善患者护理质量。

Method: 开发了基于LLM的ProFees框架，专门处理现实世界E/M编码的复杂性。

Result: 在专家策划的真实数据集上，ProFees比商业CPT E/M编码系统准确率提高超过36%，比最强的单提示基线提高近5%。

Conclusion: ProFees框架有效解决了现实世界E/M编码的复杂性，展示了LLM在医疗编码自动化中的潜力。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [59] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 本文提出了Autoregressive State-Tracking Prompting (ASTP)方法，通过显式状态追踪和占位符后处理，解决了LLM在游戏交易系统中无法遵循规则流程的问题，实现了99%以上的状态合规性和计算精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在游戏交互中具有灵活性，但无法遵循规则治理的交易系统流程（浏览-报价-审核-确认），这会损害玩家信任。需要解决LLM创意灵活性与程序化交易需求之间的核心矛盾。

Method: 引入ASTP方法，通过精心设计的提示词强制LLM显式追踪和报告预定义状态标签，结合状态特定的占位符后处理方法确保交易完整性。

Result: 在300个交易对话评估中，状态合规性>99%，计算精度99.3%。小模型(Gemini-2.5-Flash)使用ASTP后性能与大模型(Gemini-2.5-Pro)相当，响应时间从21.2秒降至2.4秒。

Conclusion: ASTP为商业游戏建立了实用基础，既能满足实时性要求，又能适应资源限制，在保持交易完整性的同时显著提升性能。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [60] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: PM4GRPO是一种推理感知的组相对策略优化方法，通过过程挖掘技术增强标准答案/格式奖励，显著提升大型推理模型的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的后训练奖励方案通常是结果导向的，缺乏对推理过程的关注，限制了大型推理模型的多步推理能力发展。

Method: 使用过程挖掘技术计算标量一致性奖励，衡量策略模型的推理过程与预训练教师模型的匹配程度，将过程信号与标准答案/格式奖励结合。

Result: 在五个基准测试上的实证结果表明，PM4GRPO显著优于现有的基于GRPO的后训练方法。

Conclusion: 利用过程挖掘进行推理感知的GRPO能有效增强策略模型的推理能力。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [61] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: 本文提出了Agentic Moderation框架，利用专门的智能体来保护多模态系统免受越狱攻击，通过动态协作的智能体实现上下文感知和可解释的内容审核。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法通常作为静态层应用于输入或输出，仅提供二元分类（安全或不安全），缺乏动态性和可解释性。本文旨在利用智能体方法的自主性和推理能力，构建更灵活、可扩展的细粒度安全执行框架。

Method: 引入Agentic Moderation框架，包含Shield、Responder、Evaluator和Reflector四个协作智能体，实现动态、上下文感知的内容审核。该框架与模型无关，可应用于各种大型视觉语言模型。

Result: 在五个数据集和四个代表性大型视觉语言模型上的广泛实验表明，该方法将攻击成功率降低7-19%，保持稳定的不遵循率，并将拒绝率提高4-20%，实现了稳健、可解释且平衡的安全性能。

Conclusion: Agentic Moderation通过利用智能体架构的灵活性和推理能力，提供了模块化、可扩展和细粒度的安全执行，展示了智能体系统作为自动化安全治理基础的广泛潜力。

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [62] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: EneAD是一个节能的自动驾驶框架，通过自适应感知模块和鲁棒决策模块，在保持感知精度的同时显著降低计算能耗，提升电动车的续航里程。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术虽然带来社会、经济和环境效益，但计算引擎的高能耗限制了电动车的续航里程。感知计算是能耗最大的组件，现有模型压缩技术往往导致模型尺寸过大或感知精度显著下降。

Method: 1. 自适应感知模块：管理多个不同计算消耗的感知模型，动态调整执行帧率；基于贝叶斯优化的可转移调优方法寻找低计算高精度的配置；轻量级分类模型区分不同场景的感知难度。2. 鲁棒决策模块：基于强化学习的决策模型，设计正则化项增强在感知结果扰动下的驾驶稳定性。

Result: EneAD能将感知能耗降低1.9倍到3.5倍，从而将驾驶里程提升3.9%到8.5%。

Conclusion: 该框架在能耗和驾驶性能方面均表现出优越性，为节能自动驾驶提供了有效解决方案。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [63] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 论文提出RAVR框架，利用答案引导的推理作为变分替代，解决LLM在强化学习中难以采样高质量推理路径的问题，在数学和通用领域均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当LLM在当前任务上能力不足时，难以生成高质量的推理路径，强化学习可能强化熟悉但次优的推理。受认知科学启发，"为什么是这个答案"比"答案是什么"更容易回答，因为避免了开放式探索的认知负担。

Method: 引入RAVR（参考答案引导的变分推理）框架，使用答案条件推理作为仅问题推理的变分替代，通过答案引导生成高质量推理路径。

Result: 在通用和数学领域的实验中，RAVR相比强基线方法取得了一致的改进，减少了犹豫，加强了结论整合，并促进了问题特定的推理策略。

Conclusion: 答案条件推理能够显著提高采样推理路径的期望效用，将难以处理的问题转化为可学习的问题，为LLM的推理能力提升提供了有效途径。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [64] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA是一个基于大语言模型的多智能体进化系统，用于从复杂的工业事件日志数据中自动提取高性能特征。它通过专门的智能体协作生成、验证和实施特征创意，结合强化学习和遗传算法原理实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 工业事件日志数据具有大规模、高维度、异构性和复杂结构等特点，使得特征工程极具挑战性。现有的自动特征工程方法存在可解释性差、操作僵化和对复杂异构数据适应性不足的问题。

Method: FELA采用多智能体架构，包括创意智能体、代码智能体和批评智能体，通过协作生成和验证特征创意。系统结合强化学习和遗传算法原理，实现智能体进化，并维护分层知识库和双记忆系统以支持持续改进。

Result: 在真实工业数据集上的实验表明，FELA能够生成可解释的、领域相关的特征，显著提升模型性能，同时减少人工工作量。

Conclusion: 基于LLM的多智能体系统有潜力成为复杂现实环境中自动化、可解释和自适应特征工程的通用框架。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [65] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 该研究开发了PsyCoTalk，首个支持共病诊断的大规模对话数据集，包含3000个多轮诊断对话，通过合成电子病历和多智能体框架生成，经精神科医生验证具有高真实性和诊断有效性。


<details>
  <summary>Details</summary>
Motivation: 精神疾病共病在临床上具有重要意义但诊断复杂，现有方法难以处理多种共病情况，需要开发能够支持共病诊断的大规模对话数据集。

Method: 采用合成患者电子病历构建和多智能体诊断对话生成的集成方法，创建502个合成EMR，将临床访谈协议转化为分层状态机和上下文树，支持130多个诊断状态。

Result: 构建了包含3000个多轮诊断对话的PsyCoTalk数据集，与真实临床记录相比，在对话长度、词元分布和诊断推理策略方面具有高结构性和语言保真度，经精神科医生确认对话真实有效。

Conclusion: PsyCoTalk数据集为精神疾病共病研究提供了宝贵资源，能够开发和评估在一次对话中完成多障碍精神疾病筛查的模型，提高了诊断准确性和治疗规划能力。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [66] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出了KVDA-UCT算法，通过放宽状态-动作对必须具有相同值的限制，允许值不同的状态-动作对进行抽象分组，只要它们的值差可以推断，从而显著提高了MCTS的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS抽象算法如OGA-UCT要求状态-动作对必须具有相同的即时奖励，这一刚性条件限制了可发现的抽象数量。为了突破这一限制，需要打破值等价分组的范式。

Method: 提出KVDA（已知值差抽象）框架，通过分析即时奖励来推断状态-动作对之间的值差异，并基于此构建抽象。将OGA-UCT修改为KVDA-UCT，使用KVDA框架进行抽象。

Result: KVDA-UCT在多种确定性环境和参数设置下显著优于OGA-UCT，能够检测到更多的抽象，且不引入额外参数。

Conclusion: KVDA框架通过允许值不同的状态-动作对进行抽象分组，有效提高了MCTS的样本效率，为MCTS抽象算法提供了新的方向。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [67] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出AI对齐研究的新视角：将工具性目标视为需要接受和管理的特征，而非需要限制的故障，并建议通过理解和管理这些目标来实现人类对齐。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐理论将工具性目标视为风险来源，试图限制其症状，但本文认为这种观点需要重新审视，以更有效地管理AI系统的行为。

Method: 借鉴亚里士多德本体论及其现代解释，构建哲学论证，将高级AI系统视为具有特定构成的人工制品，其工具性倾向是其构成的必然结果。

Result: 论证表明工具性目标是AI系统构成的固有特征，而非偶然故障，这为AI对齐提供了新的理论基础。

Conclusion: AI对齐工作应更侧重于理解、管理和引导工具性目标，而不是试图消除它们，以实现更好的人类对齐。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [68] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 多目标搜索（MOS）作为规划与决策问题的统一框架，用于平衡多个常冲突的准则。近年来在机器人、交通和运筹学等AI应用中重新受到关注，反映了现实系统很少只优化单一指标的实际情况。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统通常需要平衡多个冲突的目标，而不仅仅是优化单一指标。多目标搜索框架能够统一处理这类规划与决策问题，近年来在多个AI应用领域重新受到重视。

Method: 本文采用综述研究方法，回顾多目标搜索领域的发展，强调跨学科机会，并概述定义MOS新兴前沿的开放挑战。

Result: 论文系统梳理了多目标搜索的发展历程，识别了该领域在机器人、交通、运筹学等应用中的最新进展，并指出了跨学科合作的机会。

Conclusion: 多目标搜索作为一个统一框架，在现实世界应用中具有重要价值。该领域仍面临开放挑战，这些挑战定义了MOS的新兴前沿，需要跨学科合作来推进发展。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [69] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: MTIR-SQL是一个创新的多轮工具集成推理强化学习框架，用于Text-to-SQL任务。它通过在执行感知的多轮推理范式中无缝集成数据库执行反馈，实现上下文敏感的查询生成和渐进式优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态执行反馈，限制了实时错误纠正。集成多轮工具调用和动态反馈可以显著提高适应性和鲁棒性，从而提升模型性能。

Method: 提出MTIR-SQL框架，引入执行感知的多轮推理范式，在每一步推理中集成数据库执行反馈。扩展GRPO算法以适应复杂多轮交互场景，添加轨迹过滤机制并移除KL损失约束。

Result: MTIR-SQL在BIRD Dev上达到64.4%的准确率，在SPIDER Dev上达到84.6%的执行准确率，显著优于现有方法。

Conclusion: MTIR-SQL通过多轮工具集成推理和动态执行反馈，有效提升了Text-to-SQL任务的性能，证明了该方法在复杂交互场景中的有效性。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [70] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 使用大型语言模型为逻辑规则中的未命名谓词生成有意义的名称，以提高逻辑理论的可读性、可解释性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 在归纳逻辑编程中，各种规则生成方法会产生包含未命名谓词的规则，这阻碍了逻辑理论的可读性、可解释性和可重用性。

Method: 利用大型语言模型处理自然语言和代码的能力，为未命名谓词提供语义上有意义的命名建议。

Result: 在手工制作的逻辑规则上评估表明，大型语言模型在此任务上具有潜力。

Conclusion: 大型语言模型显示出为逻辑规则中未命名谓词生成有意义名称的潜力，有助于改善逻辑理论的质量。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [71] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体的RAG架构，通过模块化管道解决金融科技等专业领域中的检索挑战，包括智能查询重构、迭代子查询分解、上下文缩略语解析和交叉编码器重排序，在85个问答对数据集上验证了优于标准RAG基线的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在金融科技等专业领域面临挑战，包括领域特定本体、密集术语和缩略语，导致检索和合成效果不佳。

Method: 采用模块化多智能体管道，包括智能查询重构、基于关键词提取的迭代子查询分解、上下文缩略语解析和交叉编码器重排序。

Result: 在85个问答对数据集上的实验结果显示，智能体RAG系统在检索精度和相关性方面优于标准RAG基线，但延迟有所增加。

Conclusion: 结构化多智能体方法为增强复杂领域特定环境中的检索鲁棒性提供了有前景的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [72] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出了一种新的零强化学习范式，通过结合可验证奖励和生成奖励模型，在可验证和不可验证领域进行多任务训练，提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前零强化学习研究主要集中在可验证奖励信号的领域，而在验证不直接的多样化场景中激发推理能力的研究不足。

Method: 结合可验证奖励与生成奖励模型进行多任务零强化学习训练，并设计平滑长度惩罚机制来防止奖励黑客攻击。

Result: 在Qwen3-8B-Base和Qwen3-14B-Base上的实验结果表明，该方法在需要广泛推理的任务和更一般的任务上都取得了优越的推理性能。

Conclusion: 所提出的零强化学习范式能够有效提升模型在可验证和不可验证领域的推理能力，实现推理能力在不同领域间的迁移。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [73] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: 提出MoGE方法，通过生成未充分探索的关键状态和动态一致性经验来增强被动探索，包含扩散生成器和一步想象世界模型两个组件，可无缝集成现有算法提升探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决被动探索方法受限于样本多样性的问题，现有主动探索方法在高维环境中表现不佳，被动探索则受限于有限样本多样性。

Method: MoGE包含：(1)基于扩散的生成器，在效用函数指导下合成关键状态；(2)一步想象世界模型，基于关键状态构建关键转移用于智能体学习。采用模块化设计，可与现有算法无缝集成。

Result: 在OpenAI Gym和DeepMind Control Suite上的实验结果表明，MoGE有效连接探索与策略学习，在复杂控制任务中显著提升了样本效率和性能。

Conclusion: MoGE通过生成关键状态和动态一致性经验，成功解决了被动探索的局限性，为强化学习探索提供了有效解决方案。

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [74] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: CAIR是首个评估多智能体系统中各智能体对最终输出影响程度的方法，通过反事实分析提供任务无关的分析，可用于离线和推理时。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作流（AAW）的自主性增强和广泛应用，需要从质量和安全角度深入理解其运作机制，但目前缺乏评估各智能体对最终输出影响的方法。

Method: 提出基于反事实分析的智能体影响力排序器（CAIR），通过反事实分析评估每个智能体对AAW输出的影响程度，确定最具影响力的智能体。

Result: 在包含30个不同用例和230种功能的AAW数据集上评估，CAIR产生一致的排序结果，优于基线方法，并能有效提升下游任务的效果和相关性。

Conclusion: CAIR是首个能够评估多智能体系统中智能体影响力的方法，具有任务无关性，可用于离线和推理时分析，为理解和优化AAW系统提供了有效工具。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [75] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 基于深度强化学习的无人机最优导航策略，在三维城市流动环境中使用PPO+GTrXL架构，显著提高了成功率和降低了碰撞率。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在城市区域用于配送和监控的普及，需要开发在复杂城市流动环境中的最优导航策略。

Method: 采用基于深度强化学习的流感知近端策略优化(PPO)结合门控变压器超大(GTrXL)架构，为智能体提供更丰富的湍流场信息。

Result: 与PPO+LSTM、PPO+GTrXL和无次级预测任务的算法相比，该方法显著提高了成功率并降低了碰撞率。

Conclusion: 该方法为复杂城市环境中无人机导航开辟了新途径，有望重塑无人机应用格局。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [76] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: BambooKG是一种基于频率权重的知识图谱，通过在非三元组边添加权重来减少信息损失，提升单跳和多跳推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法将检索块独立处理，难以进行多跳或关系推理，特别是跨文档时。知识图谱虽然能通过三元组捕获实体关系，但会遗漏不符合三元组结构的信息。

Method: 引入BambooKG知识图谱，基于Hebbian原理在非三元组边上添加频率权重，反映链接强度。

Result: 减少了信息损失，在单跳和多跳推理任务上表现优于现有解决方案。

Conclusion: BambooKG通过频率权重机制有效提升了知识图谱在多跳推理中的性能。

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>
